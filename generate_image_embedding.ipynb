{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e48ff02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.modeling import build_model\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "import cv2\n",
    "import detectron2.data.transforms as T\n",
    "import torch\n",
    "from detectron2.structures.image_list import ImageList\n",
    "from detectron2.modeling.box_regression import Box2BoxTransform\n",
    "from detectron2.modeling.roi_heads.fast_rcnn import FastRCNNOutputs\n",
    "from detectron2.structures.boxes import Boxes\n",
    "import numpy as np\n",
    "import gc\n",
    "from detectron2.layers import nms\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b7828c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_TEST_DIR = 'data/output'\n",
    "LABELS=['easy', 'hard']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3558f981",
   "metadata": {},
   "source": [
    "# Embeddingový model\n",
    "model tvořící embeddingy byl vytvořen mírnou úpravou tohoto kódu: https://colab.research.google.com/drive/1bLGxKdldwqnMVA5x4neY7-l_8fKGWQYI?usp=sharing#scrollTo=j4Sh6otik73l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa9f879e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEmbedding():\n",
    "    def __init__(self):        \n",
    "        self.cfg_path=\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"\n",
    "        self.cfg = self.load_config_and_model_weights()\n",
    "        self.model = self.get_model()                        \n",
    "        \n",
    "    def get_image_embedding(self, img_list, batch_size):\n",
    "        img_bgr_list=[]\n",
    "        for i in img_list:\n",
    "            img_bgr_list.append(cv2.cvtColor(i, cv2.COLOR_RGB2BGR))\n",
    "        \n",
    "        images, batched_inputs = self.prepare_image_inputs(img_bgr_list)\n",
    "        \n",
    "        features = self.get_features(images)\n",
    "        proposals = self.get_proposals(images, features)\n",
    "        box_features, features_list = self.get_box_features(features, proposals, batch_size)\n",
    "        pred_class_logits, pred_proposal_deltas = self.get_prediction_logits(features_list, proposals)\n",
    "        boxes, scores, image_shapes = self.get_box_scores(pred_class_logits, pred_proposal_deltas, proposals)\n",
    "        \n",
    "        output_boxes = [self.get_output_boxes(boxes[i], batched_inputs[i], proposals[i].image_size) for i in range(len(proposals))]\n",
    "        \n",
    "        temp = [self.select_boxes(output_boxes[i], scores[i]) for i in range(len(scores))]\n",
    "        keep_boxes, max_conf = [],[]\n",
    "        for keep_box, mx_conf in temp:\n",
    "            keep_boxes.append(keep_box)\n",
    "            max_conf.append(mx_conf)\n",
    "            \n",
    "        \n",
    "        MIN_BOXES=1\n",
    "        MAX_BOXES=10\n",
    "        keep_boxes = [self.filter_boxes(keep_box, mx_conf, MIN_BOXES, MAX_BOXES) for keep_box, mx_conf in zip(keep_boxes, max_conf)]\n",
    "\n",
    "        visual_embeds = [self.get_visual_embeds(box_feature, keep_box) for box_feature, keep_box in zip(box_features, keep_boxes)]\n",
    "        \n",
    "        return visual_embeds\n",
    "    \n",
    "    def load_config_and_model_weights(self):\n",
    "        cfg = get_cfg()\n",
    "        cfg.merge_from_file(model_zoo.get_config_file(self.cfg_path))\n",
    "\n",
    "        # ROI HEADS SCORE THRESHOLD\n",
    "        cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
    "\n",
    "        cfg['MODEL']['DEVICE']='cpu'\n",
    "\n",
    "        cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(self.cfg_path)\n",
    "        return cfg\n",
    "    \n",
    "    def get_model(self):\n",
    "        # build model\n",
    "        model = build_model(self.cfg)\n",
    "\n",
    "        # load weights\n",
    "        checkpointer = DetectionCheckpointer(model)\n",
    "        checkpointer.load(self.cfg.MODEL.WEIGHTS)\n",
    "\n",
    "        # eval mode\n",
    "        model.eval()\n",
    "        return model\n",
    "    \n",
    "    def prepare_image_inputs(self, img_list):\n",
    "        cfg=self.cfg\n",
    "        # Resizing the image according to the configuration\n",
    "        transform_gen = T.ResizeShortestEdge(\n",
    "                    [cfg.INPUT.MIN_SIZE_TEST, cfg.INPUT.MIN_SIZE_TEST], cfg.INPUT.MAX_SIZE_TEST\n",
    "                )\n",
    "        img_list = [transform_gen.get_transform(img).apply_image(img) for img in img_list]\n",
    "\n",
    "        # Convert to C,H,W format\n",
    "        convert_to_tensor = lambda x: torch.Tensor(x.astype(\"float32\").transpose(2, 0, 1))\n",
    "\n",
    "        batched_inputs = [{\"image\":convert_to_tensor(img), \"height\": img.shape[0], \"width\": img.shape[1]} for img in img_list]\n",
    "\n",
    "        # Normalizing the image\n",
    "        num_channels = len(cfg.MODEL.PIXEL_MEAN)\n",
    "        pixel_mean = torch.Tensor(cfg.MODEL.PIXEL_MEAN).view(num_channels, 1, 1)\n",
    "        pixel_std = torch.Tensor(cfg.MODEL.PIXEL_STD).view(num_channels, 1, 1)\n",
    "        normalizer = lambda x: (x - pixel_mean) / pixel_std\n",
    "        images = [normalizer(x[\"image\"]) for x in batched_inputs]\n",
    "\n",
    "        # Convert to ImageList\n",
    "        images =  ImageList.from_tensors(images, self.model.backbone.size_divisibility)\n",
    "\n",
    "        return images, batched_inputs\n",
    "    \n",
    "    def get_features(self, images):\n",
    "        features = self.model.backbone(images.tensor)\n",
    "        return features\n",
    "\n",
    "    def get_proposals(self, images, features):\n",
    "        proposals, _ = self.model.proposal_generator(images, features)\n",
    "        return proposals\n",
    "\n",
    "    def get_box_features(self, features, proposals, batch_size):\n",
    "        features_list = [features[f] for f in ['p2', 'p3', 'p4', 'p5']]\n",
    "        box_features = self.model.roi_heads.box_pooler(features_list, [x.proposal_boxes for x in proposals])\n",
    "        box_features = self.model.roi_heads.box_head.flatten(box_features)\n",
    "        box_features = self.model.roi_heads.box_head.fc1(box_features)\n",
    "        box_features = self.model.roi_heads.box_head.fc_relu1(box_features)\n",
    "        box_features = self.model.roi_heads.box_head.fc2(box_features)\n",
    "        \n",
    "        padd=torch.zeros(1000*batch_size-box_features.shape[0], 1024)\n",
    "        box_features = torch.cat((box_features, padd), 0)\n",
    "        box_features = box_features.reshape(batch_size, 1000, 1024) # depends on config\n",
    "        return box_features, features_list\n",
    "    \n",
    "    def get_prediction_logits(self, features_list, proposals):\n",
    "        cls_features = self.model.roi_heads.box_pooler(features_list, [x.proposal_boxes for x in proposals])\n",
    "        cls_features = self.model.roi_heads.box_head(cls_features)\n",
    "        pred_class_logits, pred_proposal_deltas = self.model.roi_heads.box_predictor(cls_features)\n",
    "        return pred_class_logits, pred_proposal_deltas\n",
    "\n",
    "    def get_box_scores(self, pred_class_logits, pred_proposal_deltas, proposals):\n",
    "        box2box_transform = Box2BoxTransform(weights=self.cfg.MODEL.ROI_BOX_HEAD.BBOX_REG_WEIGHTS)\n",
    "        smooth_l1_beta = self.cfg.MODEL.ROI_BOX_HEAD.SMOOTH_L1_BETA\n",
    "\n",
    "        outputs = FastRCNNOutputs(\n",
    "            box2box_transform,\n",
    "            pred_class_logits,\n",
    "            pred_proposal_deltas,\n",
    "            proposals,\n",
    "            smooth_l1_beta,\n",
    "        )\n",
    "\n",
    "        boxes = outputs.predict_boxes()\n",
    "        scores = outputs.predict_probs()\n",
    "        image_shapes = outputs.image_shapes\n",
    "\n",
    "        return boxes, scores, image_shapes\n",
    "    \n",
    "    def get_output_boxes(self, boxes, batched_inputs, image_size):\n",
    "        proposal_boxes = boxes.reshape(-1, 4).clone()\n",
    "        scale_x, scale_y = (batched_inputs[\"width\"] / image_size[1], batched_inputs[\"height\"] / image_size[0])\n",
    "        output_boxes = Boxes(proposal_boxes)\n",
    "        return output_boxes\n",
    "    \n",
    "    def select_boxes(self, output_boxes, scores):\n",
    "        test_score_thresh = self.cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST\n",
    "        test_nms_thresh = self.cfg.MODEL.ROI_HEADS.NMS_THRESH_TEST\n",
    "        cls_prob = scores.detach()\n",
    "\n",
    "        out_boxes_to_padd=output_boxes.tensor.detach()\n",
    "        padd=torch.zeros(80000-out_boxes_to_padd.shape[0], 4)\n",
    "        out_boxes_padded = torch.cat((out_boxes_to_padd, padd), 0)\n",
    "        \n",
    "        cls_boxes = out_boxes_padded.reshape(1000,80,4)\n",
    "        max_conf = torch.zeros((cls_boxes.shape[0]))\n",
    "        for cls_ind in range(0, cls_prob.shape[1]-1):\n",
    "            cls_scores = cls_prob[:, cls_ind+1]\n",
    "            padd=torch.zeros(1000-len(cls_scores))\n",
    "            cls_scores=torch.cat((cls_scores, padd), 0)            \n",
    "            det_boxes = cls_boxes[:,cls_ind,:]\n",
    "            keep = np.array(nms(det_boxes, cls_scores, test_nms_thresh))\n",
    "            max_conf[keep] = torch.where(cls_scores[keep] > max_conf[keep], cls_scores[keep], max_conf[keep])\n",
    "        keep_boxes = torch.where(max_conf >= test_score_thresh)[0]\n",
    "        return keep_boxes, max_conf\n",
    "\n",
    "    def filter_boxes(self, keep_boxes, max_conf, min_boxes, max_boxes):\n",
    "        if len(keep_boxes) < min_boxes:\n",
    "            keep_boxes = np.argsort(max_conf).numpy()[::-1][:min_boxes]\n",
    "        elif len(keep_boxes) > max_boxes:\n",
    "            keep_boxes = np.argsort(max_conf).numpy()[::-1][:max_boxes]\n",
    "        return keep_boxes\n",
    "    \n",
    "    def get_visual_embeds(self, box_features, keep_boxes):\n",
    "        return box_features[keep_boxes.copy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b42f313f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_list_from_dir(data_dir):\n",
    "    imgs=[]\n",
    "    for label in range(len(LABELS)):\n",
    "        for image in os.listdir(data_dir+'/'+LABELS[label]):\n",
    "            img_path=data_dir+'/'+LABELS[label]+\"/\"+image\n",
    "            imgs.append(plt.imread(img_path))\n",
    "    return imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96bd5488",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "celá sada obrázků se nevejde do paměti, proto jsou zpracovány dávkově,\n",
    "vznikne několik souborů s embeddingy, které budou následně spojeny dohromady\n",
    "'''\n",
    "def get_img_embedding_batch(embedding_model, batch_size, img_list, filename):\n",
    "    embeddings=[]\n",
    "    cnt=0\n",
    "    for i in range(int(len(img_list)/batch_size)+1):\n",
    "        cnt+=1\n",
    "        gc.collect()\n",
    "        print(i)        \n",
    "        curr_filename=filename+\"_\"+str(i)\n",
    "        i_embeddings=embedding_model.get_image_embedding(img_list[i*batch_size:(i+1)*batch_size], batch_size)\n",
    "        torch.save(i_embeddings, curr_filename)\n",
    "    return cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20229b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "převede torch do numpy array, aby mohlo být následně použito v tensorflow modelu\n",
    "\n",
    "num_of_embed_parts... na kolik částí máme rozdělený embedding obrázků\n",
    "\n",
    "source_dir=\"data/saved/embeddings_val/val_\n",
    "\n",
    "target_dir=\"data/saved/embedds.npy\"\n",
    "'''\n",
    "def embed_to_numpy(source_dir, num_of_embed_parts, target_dir):\n",
    "    embeddings=[]\n",
    "    for i in range(num_of_embed_parts):\n",
    "        filename=source_dir+str(i)\n",
    "        t=torch.load(filename)\n",
    "        t=torch.stack(t, dim=0)\n",
    "        embeddings.append(t)\n",
    "\n",
    "    e=torch.cat(embeddings)\n",
    "    np.save(target_dir, e.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c70c1866",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir=TRAIN_TEST_DIR+\"/train\"\n",
    "train_imgs=get_img_list_from_dir(train_data_dir)\n",
    "\n",
    "val_data_dir=TRAIN_TEST_DIR+\"/val\"\n",
    "val_imgs=get_img_list_from_dir(val_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b604eb17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embed=ImageEmbedding()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e73e773",
   "metadata": {},
   "source": [
    "Vytvoření embeddingů pro obě sady obrázků a jejich převod do numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35428bc7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size=10\n",
    "my_dir=\"data/saved/embeddings_train/\"\n",
    "filename=my_dir+\"train\"\n",
    "if not os.path.exists(my_dir):\n",
    "    os.makedirs(my_dir) \n",
    "    \n",
    "train_batch_cnt=get_img_embedding_batch(embed, batch_size, train_imgs, filename)\n",
    "embed_to_numpy(my_dir+\"train_\", train_batch_cnt, \"data/saved/train_embedds.npy\")\n",
    "\n",
    "my_dir=\"data/saved/embeddings_val/\"\n",
    "filename=my_dir+\"val\"\n",
    "if not os.path.exists(my_dir):\n",
    "    os.makedirs(my_dir) \n",
    "\n",
    "val_batch_cnt=get_img_embedding_batch(embed, batch_size, val_imgs, filename)\n",
    "embed_to_numpy(my_dir+\"val_\", val_batch_cnt, \"data/saved/val_embedds.npy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
